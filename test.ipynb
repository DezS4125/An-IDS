{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4be89d3-0c83-47ec-b946-16a869fcd8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # for array\n",
    "import pandas as pd  # for csv files and dataframe\n",
    "import matplotlib.pyplot as plt  # for plotting\n",
    "import seaborn as sns  # plotting\n",
    "from scipy import stats\n",
    "from tqdm import tqdm  # Progress bar\n",
    "\n",
    "import pickle  # To load data int disk\n",
    "from prettytable import PrettyTable  # To print in tabular format\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler  # Standardizer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder  # One hot Encoder\n",
    "from scipy.sparse import csr_matrix  # For sparse matrix\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Different Models\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier  # LR\n",
    "from sklearn.svm import LinearSVC  # SVM\n",
    "from sklearn.tree import DecisionTreeClassifier  #DT\n",
    "from sklearn.ensemble import RandomForestClassifier  # RF\n",
    "import xgboost as xgb  #XGB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, make_scorer  # Scoring functions\n",
    "from sklearn.metrics import auc, f1_score, roc_curve, roc_auc_score  # Scoring fns\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV  # Cross validation\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "from sklearn import metrics\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aadde4cd-7270-4789-a4cb-8190755449a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"./datasets/final_UNSW-NB15\"\n",
    "# Train and Test data\n",
    "x_train, y_train = pickle.load(open(file_path+'/final_train.pkl', 'rb'))\n",
    "x_test, y_test = pickle.load(open(file_path+'/final_test.pkl', 'rb'))\n",
    "\n",
    "# Dictionaries\n",
    "saved_dict = pickle.load(open(file_path+'/saved_dict.pkl', 'rb'))\n",
    "mode_dict = pickle.load(open(file_path+'/mode_dict.pkl', 'rb'))\n",
    "\n",
    "# Standard scaler\n",
    "scaler = pickle.load(open(file_path+'/scaler.pkl', 'rb'))\n",
    "\n",
    "# Onehot encoders\n",
    "ohe_proto = pickle.load(open(file_path+'/ohe_proto.pkl', 'rb'))\n",
    "ohe_service = pickle.load(open(file_path+'/ohe_service.pkl', 'rb'))\n",
    "ohe_state = pickle.load(open(file_path+'/ohe_state.pkl', 'rb'))\n",
    "\n",
    "label_encoder = pickle.load(open(file_path+'/label_encoder.pkl', 'rb'))\n",
    "clf = pickle.load(open('datasets/final_UNSW-NB15/my_ensemble_clf.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dfe83fb-bc52-4208-ad7d-2b68dd89b8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------\n",
    "# Data Cleaning\n",
    "#------------------------------------------------------------------------------------------\n",
    "def clean_data(data):\n",
    "    '''\n",
    "    Cleans given raw data. Performs various cleaning, removes Null and wrong values.\n",
    "    Check for columns datatype and fix them.\n",
    "    '''\n",
    "    numerical_col = data.select_dtypes(include=np.number).columns  # All the numerical columns list\n",
    "    categorical_col = data.select_dtypes(exclude=np.number).columns  # All the categorical columns list\n",
    "    \n",
    "    # Cleaning the data\n",
    "    for col in data.columns:\n",
    "        val = mode_dict[col]  # Mode value of the column in train data\n",
    "        data[col] = data[col].fillna(value=val)\n",
    "        data[col] = data[col].replace(' ', value=val)\n",
    "        data[col] = data[col].apply(lambda x:\"none\" if x==\"-\" else x)\n",
    "\n",
    "        # Fixing binary columns\n",
    "        if col in saved_dict['binary_col']:\n",
    "            data[col] = np.where(data[col]>1, val, data[col])\n",
    "\n",
    "    # Fixing datatype of columns\n",
    "    bad_dtypes = list(set(categorical_col) - set(saved_dict['cat_col']))\n",
    "    for bad_col in bad_dtypes:\n",
    "        data[col] = data[col].astype(float)\n",
    "    \n",
    "    return data\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "# Feature Engineering: Apply log1p\n",
    "#------------------------------------------------------------------------------------------\n",
    "def apply_log1p(data):\n",
    "    '''\n",
    "    Performs FE on the data. Apply log1p on the specified columns create new column and remove those original columns.\n",
    "    '''\n",
    "    for col in saved_dict['log1p_col']:\n",
    "        new_col = col + '_log1p'  # New col name\n",
    "        data[new_col] = data[col].apply(np.log1p)  # Creating new column on transformed data\n",
    "        data.drop(col, axis=1, inplace=True)  # Removing old columns\n",
    "    return data\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "# Standardizing: Mean centering an d varience scaling\n",
    "#------------------------------------------------------------------------------------------\n",
    "def standardize(data):\n",
    "    '''\n",
    "    Stanardize the given data. Performs mean centering and varience scaling.\n",
    "    Using stanardscaler object trained on train data.\n",
    "    '''\n",
    "    data[saved_dict['num_col']] = scaler.transform(data[saved_dict['num_col']])\n",
    "    return data\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "# Onehot encoding of categorical columns\n",
    "#------------------------------------------------------------------------------------------\n",
    "def ohencoding(data):\n",
    "    '''\n",
    "    Onehot encoding the categoricla columns.\n",
    "    Add the ohe columns with the data and removes categorical columns.\n",
    "    Using Onehotencoder objects trained on train data.\n",
    "    '''\n",
    "    # Onehot encoding cat col using onehotencoder objects\n",
    "    X = ohe_service.transform(data['service'].values.reshape(-1, 1))\n",
    "    Xm = ohe_proto.transform(data['proto'].values.reshape(-1, 1))\n",
    "    Xmm = ohe_state.transform(data['state'].values.reshape(-1, 1))\n",
    "    \n",
    "    # Adding encoding data to original data\n",
    "    data = pd.concat([data,\n",
    "                      pd.DataFrame(Xm.toarray(), columns=['proto_'+i for i in ohe_proto.categories_[0]]),\n",
    "                      pd.DataFrame(X.toarray(), columns=['service_'+i for i in ohe_service.categories_[0]]),\n",
    "                      pd.DataFrame(Xmm.toarray(), columns=['state_'+i for i in ohe_state.categories_[0]])],\n",
    "                      axis=1)\n",
    "    \n",
    "    # Removing cat columns\n",
    "    data.drop(['proto', 'service', 'state'], axis=1, inplace=True)\n",
    "    return data\n",
    "def labelcoding(data):\n",
    "    data= label_encoder.transform(data) \n",
    "    return data\n",
    "def get_final_data(data, saved_dict=saved_dict, mode_dict=mode_dict):\n",
    "    '''\n",
    "    This functions takes raw input and convert that to model required output.\n",
    "    '''\n",
    "    data.reset_index(drop=True, inplace=True)\n",
    "    data.columns = saved_dict['columns']\n",
    "    \n",
    "    data['network_bytes'] = data['dbytes'] + data['sbytes']\n",
    "    \n",
    "    dropable_col = saved_dict['to_drop'] + saved_dict['corr_col']\n",
    "    data.drop(columns=dropable_col, inplace=True)\n",
    "    \n",
    "    data = clean_data(data)\n",
    "    data = apply_log1p(data)\n",
    "    data = standardize(data)\n",
    "    data = ohencoding(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fe71ecd-3fdb-45bd-a029-71a37a471fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pipeline to prepare test data\n",
    "x_test = get_final_data(x_test)\n",
    "y_test=y_test.fillna(value=\"normal\").apply(lambda x:x.strip().lower())\n",
    "y_test = y_test.replace('backdoors','backdoor', regex=True).apply(lambda x: x.strip().lower())\n",
    "y_test= labelcoding(y_test)\n",
    "y_test = pd.DataFrame(y_test,columns=['attack_cat',],)\n",
    "y_test=y_test['attack_cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de6a0ab2-7ffa-4dfa-be6b-588ababc50d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762015, 197)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "59ce5709-e067-46e5-94bf-6be3e4292401",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(762015,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3deb817c-73d1-4bdc-b7ac-860f85eb477f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0\n",
       "0  6\n",
       "1  6\n",
       "2  6\n",
       "3  6\n",
       "4  6\n",
       "5  6\n",
       "6  6\n",
       "7  6\n",
       "8  5\n",
       "9  6"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(clf.predict(x_test.tail(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "467cec60-9bc2-431d-9d0b-d4a594cacf09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762005    6\n",
       "762006    6\n",
       "762007    6\n",
       "762008    6\n",
       "762009    6\n",
       "762010    6\n",
       "762011    6\n",
       "762012    6\n",
       "762013    5\n",
       "762014    6\n",
       "Name: attack_cat, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe5a9045-5010-44c8-9429-179e9c67bdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1778032,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8195323c-61b0-45b1-bb73-417ee0ba1666",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
